"use server";

import OpenAI from "openai";
import { z } from "zod";

import { Identification } from "@/stores/identification-store";
import { Widget } from "@/stores/widget-store";
import { getUser } from "@/utils/supabase/server";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
});

// TODO add dataset description can be generated by llm e.g. "This dataset
// contains the latitude, longitude, population, and country for every city in
// the world." include in the prompt as:
// # Dataset Description
// ${datasetDescription} this will help the model understand the data and
// generate a better visualization.

// TODO would sonnet do a better job if it simulated a reasoning chain? we can
// use r1 to see a reasoning chain.

// After ~ 100 tests, model quality is:
// best: o1, r1
// good: sonnet
// bad: o1-mini, gpt-4o

const OPENAI_MODEL_O1 = "o1-mini"; // works a lot better with o1-preview; but saving $$$
const OPENAI_MODEL_STRUCTURED = "gpt-4o-mini";

export interface SuggestWidgetColumn {
  fieldName: string;
  identification: Identification;
  sampleValues: string[];
}

const widgetSuggestionSchema = z.object({
  name: z.string(),
  description: z.string(),
  vegaLiteSpec: z.record(z.any()),
});

type WidgetSuggestion = z.infer<typeof widgetSuggestionSchema>;

export async function suggestWidget(
  columns: SuggestWidgetColumn[],
  existingWidgets: Widget[],
  dataSize: number
): Promise<WidgetSuggestion> {
  const { user } = await getUser();
  if (!user) throw new Error("Not authenticated");

  if (columns.length > 30) {
    throw new Error("Too many columns. Please limit to 30 columns.");
  }

  const prompt = `
Given the following dataset columns with their types and sample values,
suggest a meaningful Vega-Lite visualization specification.

# Rules

1. Be creative to provide one interesting, useful, concise, and intuitive
   visualization.

2. It's OK to provide a targeted visualization of a particular aspect of the
   data -- assume that multiple visualizations will be created. For example,
   a histogram of a particular column is a good idea if it's interesting.

3. Do not suggest a visualization that is equivalent to one of the existing
   ones listed in the Existing Visualizations section.

4. The field names in the Vega-Lite spec must match exactly the fieldNames in
   the Columns section.

5. The vegaLiteSpec should be a valid Vega-Lite specification in JSON format.

6. Each column will have ${dataSize} values. Be sure that the visualization
   can render in a reasonable amount of time for this data size, and that the
   visual elements will not overlap or becomes unreadable.  For example, do
   not include more than ~ 40 labels on the x-axis or y-axis or in the
   legend. Do not include more than ~ 400 marks.

7. The final visualization will be rendered in a 385px width and 300px height
   container.

8. The response should be a valid JSON object. It should have the format:

{
  "name": "...",
  "description": "...",
  "vegaLiteSpec": { ... }
}

IT IS VERY IMPORTANT THAT THE RESPONSE IS A ONLY VALID JSON OBJECT.

# Columns

${JSON.stringify(columns, null, 2)}

# Existing Visualizations

${JSON.stringify(
  existingWidgets.map((w) => ({
    name: w.name,
    description: w.description,
  })),
  null,
  2
)}
  `;

  console.log(
    `ü§ñ Step 1: Querying ${OPENAI_MODEL_O1} for visualization suggestion...`
  );
  console.log("Prompt:", prompt);

  let o1Response: string;
  try {
    const completion = await openai.chat.completions.create({
      messages: [{ role: "user", content: prompt }],
      model: OPENAI_MODEL_O1,
    });
    const res = completion.choices[0]?.message?.content;
    if (!res) throw new Error(`No response from OpenAI ${OPENAI_MODEL_O1}`);
    o1Response = res;
    console.log("‚úÖ ${OPENAI_MODEL_O1} response received:", o1Response);
    console.log("üìä Token usage for ${OPENAI_MODEL_O1}:", {
      prompt_tokens: completion.usage?.prompt_tokens ?? 0,
      completion_tokens: completion.usage?.completion_tokens ?? 0,
      total_tokens: completion.usage?.total_tokens ?? 0,
    });
  } catch (error) {
    console.error(`‚ùå Error getting ${OPENAI_MODEL_O1} response:`, error);
    throw new Error(
      `Failed to generate visualization suggestion with ${OPENAI_MODEL_O1}`
    );
  }

  console.log(
    `ü§ñ Step 2: Processing with ${OPENAI_MODEL_STRUCTURED} for structured output...`
  );

  try {
    const structuredResponse = await openai.chat.completions.create({
      messages: [
        {
          role: "user",
          content: `Given the following data, format it as a valid JSON object with the following structure:
          {
            "name": string,
            "description": string,
            "vegaLiteSpec": object
          }

          Here is the data to format: ${o1Response}`,
        },
      ],
      model: OPENAI_MODEL_STRUCTURED,
      response_format: { type: "json_object" },
    });

    const res = structuredResponse.choices[0]?.message?.content;
    if (!res) {
      throw new Error(`No structured response from ${OPENAI_MODEL_STRUCTURED}`);
    }

    console.log("üìä Token usage for ${OPENAI_MODEL_STRUCTURED}:", {
      prompt_tokens: structuredResponse.usage?.prompt_tokens ?? 0,
      completion_tokens: structuredResponse.usage?.completion_tokens ?? 0,
      total_tokens: structuredResponse.usage?.total_tokens ?? 0,
    });

    const parsed = JSON.parse(res);
    const parseResult = widgetSuggestionSchema.safeParse(parsed);
    if (!parseResult.success) {
      throw new Error(
        `Invalid response format from ${OPENAI_MODEL_STRUCTURED}`
      );
    }

    console.log("‚úÖ Final structured suggestion:", parseResult.data);
    return parseResult.data;
  } catch (error) {
    console.error("‚ùå Error processing structured output:", error);
    throw new Error(
      `Failed to process structured output with ${OPENAI_MODEL_STRUCTURED}`
    );
  }
}
