"use server";

import { z } from "zod";

import { ChatAnthropic, ChatAnthropicCallOptions } from "@langchain/anthropic";
import { BaseLanguageModelInput } from "@langchain/core/language_models/base";
import { AIMessageChunk } from "@langchain/core/messages";
import { Runnable } from "@langchain/core/runnables";
import { ChatOpenAI, ChatOpenAICallOptions } from "@langchain/openai";

import { Identification } from "@/stores/identification-store";
import { Widget } from "@/stores/widget-store";
import { getUser } from "@/utils/supabase/server";

// TODO add dataset description can be generated by llm e.g. "This dataset
// contains the latitude, longitude, population, and country for every city in
// the world." include in the prompt as:
// # Dataset Description
// ${datasetDescription} this will help the model understand the data and
// generate a better visualization.

// TODO would sonnet do a better job if it simulated a reasoning chain? we can
// use r1 to see a reasoning chain.

// After ~ 100 tests, model quality is:
// best: o1, r1, 03-mini-high
// good: sonnet, 03-mini  <- will be fun to set these up head to head
// bad: o1-mini, gpt-4o, gemini advanced

interface LLMConfig {
  provider: "openai" | "anthropic";
  modelName: "gpt-4o-mini" | "claude-3-5-sonnet-latest" | "o3-mini";
  mode?: "structured";
  reasoning_effort?: "low" | "medium" | "high";
}

const inferenceLLMConfig: LLMConfig = {
  provider: "openai",
  modelName: "o3-mini",
  reasoning_effort: "high",
};

const structuredLLMConfig: LLMConfig = {
  provider: "openai",
  modelName: "gpt-4o-mini",
  mode: "structured",
};

// Create LLM instance based on provider
function createLLM(
  config: LLMConfig
): Runnable<
  BaseLanguageModelInput,
  AIMessageChunk,
  ChatOpenAICallOptions | ChatAnthropicCallOptions
> {
  switch (config.provider) {
    case "openai":
      const llm = new ChatOpenAI({
        modelName: config.modelName,
        ...(config.reasoning_effort && {
          reasoning_effort: config.reasoning_effort,
        }),
      });
      if (config.mode === "structured") {
        return llm.bind({
          response_format: { type: "json_object" },
        });
      }
      return llm;
    case "anthropic":
      return new ChatAnthropic({
        modelName: config.modelName,
      });
    default:
      throw new Error(`Unsupported LLM provider: ${config.provider}`);
  }
}

export interface SuggestWidgetColumn {
  fieldName: string;
  identification: Identification;
  sampleValues: string[];
}

const widgetSuggestionSchema = z.object({
  name: z.string(),
  description: z.string(),
  vegaLiteSpec: z.record(z.any()),
});

type WidgetSuggestion = z.infer<typeof widgetSuggestionSchema>;

export async function suggestWidget(
  columns: SuggestWidgetColumn[],
  existingWidgets: Widget[],
  dataSize: number
): Promise<WidgetSuggestion> {
  const { user } = await getUser();
  if (!user) throw new Error("Not authenticated");

  if (columns.length > 30) {
    throw new Error("Too many columns. Please limit to 30 columns.");
  }

  const prompt = `
Given the following dataset columns with their types and sample values,
suggest a meaningful Vega-Lite visualization specification.

# Rules

1. Be creative to provide one interesting, useful, concise, and intuitive
   visualization.

2. It's OK to provide a targeted visualization of a particular aspect of the
   data -- assume that multiple visualizations will be created. For example,
   a histogram of a particular column is a good idea if it's interesting.

3. Do not suggest a visualization that is equivalent to one of the existing
   ones listed in the Existing Visualizations section.

4. The field names in the Vega-Lite spec must match exactly the fieldNames in
   the Columns section.

5. The vegaLiteSpec should be a valid Vega-Lite specification in JSON format.

6. Each column will have ${dataSize} values. Be sure that the visualization
   can render in a reasonable amount of time for this data size, and that the
   visual elements will not overlap or becomes unreadable.  For example, do
   not include more than ~ 40 labels on the x-axis or y-axis or in the
   legend. Do not include more than ~ 400 marks.

7. The final visualization will be rendered in a 385px width and 300px height
   container.

8. The response should be a valid JSON object. It should have the format:

{
  "name": "...",
  "description": "...",
  "vegaLiteSpec": { ... }
}

IT IS VERY IMPORTANT THAT THE RESPONSE IS A ONLY VALID JSON OBJECT.

# Columns

${JSON.stringify(columns, null, 2)}

# Existing Visualizations

${JSON.stringify(
  existingWidgets.map((w) => ({
    name: w.name,
    description: w.description,
  })),
  null,
  2
)}
  `;

  console.log(
    `ü§ñ Step 1: Querying ${inferenceLLMConfig.modelName} for visualization suggestion...` +
      (inferenceLLMConfig.reasoning_effort
        ? ` (reasoning_effort: ${inferenceLLMConfig.reasoning_effort})`
        : "")
  );
  // console.log("Prompt:", prompt);

  try {
    // Step 1: Get initial suggestion
    const llm = createLLM(inferenceLLMConfig);
    const initialResponse = await llm.invoke(prompt);
    const initialSuggestion = initialResponse.content.toString();

    console.log("‚úÖ Initial LLM response received:", initialSuggestion);

    // We only need this if the model is bad at returning JSON (e.g. o1-mini):

    // // Step 2: Process with structured output
    // const structuredLLM = createLLM(structuredLLMConfig);
    // const structuredPrompt = `Given the following data, format it as a valid JSON object with the following structure:
    // {
    //   "name": string,
    //   "description": string,
    //   "vegaLiteSpec": object
    // }

    // Here is the data to format: ${initialSuggestion}`;

    // const structuredResponse = await structuredLLM.invoke(structuredPrompt);
    // const structuredSuggestion = structuredResponse.content.toString();

    // console.log("‚úÖ Final structured suggestion:", structuredSuggestion);

    // const parsed = JSON.parse(structuredSuggestion);

    const parsed = JSON.parse(initialSuggestion);
    const parseResult = widgetSuggestionSchema.safeParse(parsed);

    if (!parseResult.success) {
      throw new Error(
        `Invalid response format from ${structuredLLMConfig.modelName}`
      );
    }

    return parseResult.data;
  } catch (error) {
    console.error("‚ùå Error generating visualization suggestion:", error);
    throw new Error(
      `Failed to generate visualization suggestion with ${structuredLLMConfig.modelName}`
    );
  }
}
